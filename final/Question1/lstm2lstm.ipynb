{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string,re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Tokenization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Building the model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')[:100000]\n",
    "valid = pd.read_csv('Data/valid.csv')\n",
    "test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Data Cleaning `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Missing values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data: \")\n",
    "print(train.isna().sum())\n",
    "\n",
    "print(\"\\nValidation Data: \")\n",
    "print(valid.isna().sum())\n",
    "\n",
    "print(\"\\nTest Data: \")\n",
    "print(test.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Duplicated values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data: \", train.duplicated().sum())\n",
    "\n",
    "print(\"\\nValidation Data: \", valid.duplicated().sum())\n",
    "\n",
    "print(\"\\nTest Data: \", test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceCleaning(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # remove punctuation\n",
    "    dataframe['en'] = dataframe['en'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) \n",
    "    dataframe['vi'] = dataframe['vi'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # reduce vocab size \n",
    "    dataframe['en'] = dataframe['en'].str.lower() \n",
    "    dataframe['vi'] = dataframe['vi'].str.lower()\n",
    "    \n",
    "    # clear spaces in the beginning and end\n",
    "    dataframe['en'] = dataframe['en'].str.strip() \n",
    "    dataframe['vi'] = dataframe['vi'].str.strip()\n",
    "\n",
    "    # reduce multiple spaces to single space\n",
    "    dataframe['en'] = dataframe['en'].apply(lambda x: re.sub('\\s+',' ',x)) \n",
    "    dataframe['vi'] = dataframe['vi'].apply(lambda x: re.sub('\\s+',' ',x))\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SentenceCleaning(train)\n",
    "valid = SentenceCleaning(valid)\n",
    "test = SentenceCleaning(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_filter(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    def is_valid_language_sentence(sentence) -> bool:\n",
    "        return bool(re.compile(r'^[A-Za-zÀ-ỹà-ỹ0-9\\s]*$').match(sentence))\n",
    "        \n",
    "    filtered_df = dataframe[dataframe['en'].apply(is_valid_language_sentence) & dataframe['vi'].apply(is_valid_language_sentence)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before filtering: \")\n",
    "print(f'Train: {train.shape[0]}')\n",
    "print(f'valid: {valid.shape[0]}')\n",
    "print(f'test: {test.shape[0]}')\n",
    "\n",
    "train = sentence_filter(train)\n",
    "valid = sentence_filter(valid)\n",
    "test = sentence_filter(test)\n",
    "\n",
    "print(\"\\nAfter filtering: \")\n",
    "print(f'Train: {train.shape[0]}')\n",
    "print(f'valid: {valid.shape[0]}')\n",
    "print(f'test: {test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "vi_tokenizer = Tokenizer()\n",
    "en_tokenizer = Tokenizer()\n",
    "\n",
    "vi_tokenizer.fit_on_texts(train['vi'])\n",
    "en_tokenizer.fit_on_texts(train['en'])\n",
    "\n",
    "vi_sequences = vi_tokenizer.texts_to_sequences(train['vi'])\n",
    "en_sequences = en_tokenizer.texts_to_sequences(train['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_vi_len = max([len(seq) for seq in vi_sequences])\n",
    "max_en_len = max([len(seq) for seq in en_sequences])\n",
    "\n",
    "vi_sequences = pad_sequences(vi_sequences, maxlen=max_vi_len, padding='post')\n",
    "en_sequences = pad_sequences(en_sequences, maxlen=max_en_len, padding='post')\n",
    "\n",
    "vi_valid_sequences = vi_tokenizer.texts_to_sequences(valid['vi'])\n",
    "en_valid_sequences = en_tokenizer.texts_to_sequences(valid['en'])\n",
    "vi_valid_sequences = pad_sequences(vi_valid_sequences, maxlen=max_vi_len, padding='post')\n",
    "en_valid_sequences = pad_sequences(en_valid_sequences, maxlen=max_en_len, padding='post')\n",
    "\n",
    "vi_test_sequences = vi_tokenizer.texts_to_sequences(test['vi'])\n",
    "en_test_sequences = en_tokenizer.texts_to_sequences(test['en'])\n",
    "vi_test_sequences = pad_sequences(vi_test_sequences, maxlen=max_vi_len, padding='post')\n",
    "en_test_sequences = pad_sequences(en_test_sequences, maxlen=max_en_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, vi_sequences, en_sequences):\n",
    "        self.vi_sequences = vi_sequences\n",
    "        self.en_sequences = en_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vi_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.vi_sequences[idx], dtype=torch.long), torch.tensor(self.en_sequences[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of data\n",
    "train_dataset = TranslationDataset(vi_sequences[:10000], en_sequences[:10000])  # Use a subset of 10,000 samples\n",
    "valid_dataset = TranslationDataset(vi_valid_sequences[:2000], en_valid_sequences[:2000])\n",
    "test_dataset = TranslationDataset(vi_test_sequences[:2000], en_test_sequences[:2000])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Train models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define constants\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Decoder without attention\n",
    "class DecoderNoAttention(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_size):\n",
    "        super(DecoderNoAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, tgt, hidden, cell):\n",
    "        embedded = self.embedding(tgt)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "# Decoder with Bahdanau Attention\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden[-1].unsqueeze(1)  # Take the last layer of hidden states\n",
    "        scores = torch.tanh(self.attn(torch.cat((hidden.expand_as(encoder_outputs), encoder_outputs), dim=2)))\n",
    "        scores = torch.sum(self.v * scores, dim=2)\n",
    "        attn_weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_size):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    def forward(self, tgt, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(tgt)\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        lstm_input = torch.cat((embedded, context.unsqueeze(1).expand_as(embedded)), dim=2)\n",
    "        outputs, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        predictions = self.fc(torch.cat((outputs, context.unsqueeze(1).expand_as(outputs)), dim=2))\n",
    "        return predictions, hidden, cell, attn_weights\n",
    "\n",
    "# Training loop\n",
    "def train_model(encoder, decoder, dataloader, criterion, encoder_optimizer, decoder_optimizer, num_epochs=20):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            encoder_outputs, hidden, cell = encoder(src)\n",
    "            decoder_input = tgt[:, :-1]\n",
    "            decoder_target = tgt[:, 1:]\n",
    "\n",
    "            if isinstance(decoder, DecoderNoAttention):\n",
    "                predictions, _, _ = decoder(decoder_input, hidden, cell)\n",
    "            else:\n",
    "                predictions, _, _, _ = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "\n",
    "            # Compute loss\n",
    "            predictions = predictions.permute(0, 2, 1)\n",
    "            loss = criterion(predictions, decoder_target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}')\n",
    "\n",
    "# Create models\n",
    "input_dim = len(vi_tokenizer.word_index) + 1\n",
    "output_dim = len(en_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_size).to(device)\n",
    "decoder_no_attention = DecoderNoAttention(output_dim, embedding_dim, hidden_size).to(device)\n",
    "decoder_with_attention = DecoderWithAttention(output_dim, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer_no_attention = optim.Adam(decoder_no_attention.parameters(), lr=0.001)\n",
    "decoder_optimizer_with_attention = optim.Adam(decoder_with_attention.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(encoder, decoder_no_attention, train_loader, criterion, encoder_optimizer, decoder_optimizer_no_attention, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with attention\n",
    "train_model(encoder, decoder_with_attention, train_loader, criterion, encoder_optimizer, decoder_optimizer_with_attention, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(encoder, decoder, dataloader, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    epoch_loss = 0\n",
    "    bleu_scores = []\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
    "    meteor_scores = []\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            encoder_outputs, hidden, cell = encoder(src)\n",
    "            decoder_input = tgt[:, :-1]\n",
    "            decoder_target = tgt[:, 1:]\n",
    "\n",
    "            if isinstance(decoder, DecoderNoAttention):\n",
    "                predictions, _, _ = decoder(decoder_input, hidden, cell)\n",
    "            else:\n",
    "                predictions, _, _, _ = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "\n",
    "            predictions = predictions.permute(0, 2, 1)\n",
    "            loss = criterion(predictions, decoder_target)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate BLEU, ROUGE, and METEOR scores\n",
    "            for i in range(predictions.size(0)):\n",
    "                reference = tgt[i, 1:].cpu().numpy()\n",
    "                candidate = predictions[i].argmax(0).cpu().numpy()\n",
    "                bleu_scores.append(sentence_bleu([reference], candidate, smoothing_function=smoothing_function))\n",
    "                rouge_score = rouge.get_scores(' '.join(map(str, candidate)), ' '.join(map(str, reference)))[0]\n",
    "                rouge_scores['rouge-1'].append(rouge_score['rouge-1']['f'])\n",
    "                rouge_scores['rouge-2'].append(rouge_score['rouge-2']['f'])\n",
    "                rouge_scores['rouge-l'].append(rouge_score['rouge-l']['f'])\n",
    "                meteor_scores.append(meteor_score([' '.join(map(str, reference))], ' '.join(map(str, candidate))))\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    avg_rouge = {k: np.mean(v) for k, v in rouge_scores.items()}\n",
    "    avg_meteor = np.mean(meteor_scores)\n",
    "\n",
    "    return avg_loss, avg_bleu, avg_rouge, avg_meteor\n",
    "\n",
    "# Evaluate models without attention\n",
    "loss_no_attention, bleu_no_attention, rouge_no_attention, meteor_no_attention = evaluate_model(encoder, decoder_no_attention, test_loader, criterion)\n",
    "\n",
    "# Evaluate models with attention\n",
    "loss_with_attention, bleu_with_attention, rouge_with_attention, meteor_with_attention = evaluate_model(encoder, decoder_with_attention, test_loader, criterion)\n",
    "\n",
    "print(f'Loss without attention: {loss_no_attention}')\n",
    "print(f'BLEU score without attention: {bleu_no_attention}')\n",
    "print(f'ROUGE-1 score without attention: {rouge_no_attention[\"rouge-1\"]}')\n",
    "print(f'ROUGE-2 score without attention: {rouge_no_attention[\"rouge-2\"]}')\n",
    "print(f'ROUGE-L score without attention: {rouge_no_attention[\"rouge-l\"]}')\n",
    "print(f'METEOR score without attention: {meteor_no_attention}')\n",
    "\n",
    "print(f'Loss with attention: {loss_with_attention}')\n",
    "print(f'BLEU score with attention: {bleu_with_attention}')\n",
    "print(f'ROUGE-1 score with attention: {rouge_with_attention[\"rouge-1\"]}')\n",
    "print(f'ROUGE-2 score with attention: {rouge_with_attention[\"rouge-2\"]}')\n",
    "print(f'ROUGE-L score with attention: {rouge_with_attention[\"rouge-l\"]}')\n",
    "print(f'METEOR score with attention: {meteor_with_attention}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_attention(attention_weights, input_sentence, output_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(attention_weights, xticklabels=output_sentence, yticklabels=input_sentence, cmap='viridis')\n",
    "    plt.xlabel('Output Sentence')\n",
    "    plt.ylabel('Input Sentence')\n",
    "    plt.show()\n",
    "\n",
    "# Example sentence\n",
    "example_src, example_tgt = next(iter(test_loader))\n",
    "example_src, example_tgt = example_src[0].unsqueeze(0).to(device), example_tgt[0].unsqueeze(0).to(device)\n",
    "\n",
    "# Get attention weights for Bahdanau attention model\n",
    "encoder.eval()\n",
    "decoder_with_attention.eval()\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, hidden, cell = encoder(example_src)\n",
    "    decoder_input = example_tgt[:, :-1]\n",
    "    predictions, _, _, attention_weights = decoder_with_attention(decoder_input, hidden, cell, encoder_outputs)\n",
    "\n",
    "# Plot attention\n",
    "input_sentence = [vi_tokenizer.index_word[idx.item()] for idx in example_src[0] if idx.item() != 0]\n",
    "output_sentence = [en_tokenizer.index_word[idx.item()] for idx in example_tgt[0] if idx.item() != 0]\n",
    "plot_attention(attention_weights[0].cpu().numpy(), input_sentence, output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
