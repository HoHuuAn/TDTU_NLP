{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `521H0489 - Hồ Hữu An`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Unigram - Bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "UNK = None\n",
    "SENTENCE_START = \"<s>\"\n",
    "SENTENCE_END = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [re.split(\"\\s+\", line.rstrip('\\n')) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Unigram model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramLanguageModel:\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        self.unigram_frequencies = dict()\n",
    "        self.corpus_length = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigram_frequencies[word] = self.unigram_frequencies.get(word, 0) + 1\n",
    "                if word != SENTENCE_START and word != SENTENCE_END:\n",
    "                    self.corpus_length += 1\n",
    "        # subtract 2 because unigram_frequencies dictionary contains values for SENTENCE_START and SENTENCE_END\n",
    "        self.unique_words = len(self.unigram_frequencies) - 2\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def calculate_unigram_probability(self, word):\n",
    "            word_probability_numerator = self.unigram_frequencies.get(word, 0)\n",
    "            word_probability_denominator = self.corpus_length\n",
    "            if self.smoothing:\n",
    "                word_probability_numerator += 1\n",
    "                # add one more to total number of seen unique words for UNK - unseen events\n",
    "                word_probability_denominator += self.unique_words + 1\n",
    "            return float(word_probability_numerator) / float(word_probability_denominator)\n",
    "\n",
    "    def calculate_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        sentence_probability_log_sum = 0\n",
    "        for word in sentence:\n",
    "            if word != SENTENCE_START and word != SENTENCE_END:\n",
    "                word_probability = self.calculate_unigram_probability(word)\n",
    "                sentence_probability_log_sum += math.log(word_probability, 2)\n",
    "        return math.pow(2, sentence_probability_log_sum) if normalize_probability else sentence_probability_log_sum                \n",
    "\n",
    "    def sorted_vocabulary(self):\n",
    "        full_vocab = list(self.unigram_frequencies.keys())\n",
    "        full_vocab.remove(SENTENCE_START)\n",
    "        full_vocab.remove(SENTENCE_END)\n",
    "        full_vocab.sort()\n",
    "        full_vocab.append(UNK)\n",
    "        full_vocab.append(SENTENCE_START)\n",
    "        full_vocab.append(SENTENCE_END)\n",
    "        return full_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Bigram model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(UnigramLanguageModel):\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        UnigramLanguageModel.__init__(self, sentences, smoothing)\n",
    "        self.bigram_frequencies = dict()\n",
    "        self.unique_bigrams = set()\n",
    "        for sentence in sentences:\n",
    "            previous_word = None\n",
    "            for word in sentence:\n",
    "                if previous_word != None:\n",
    "                    self.bigram_frequencies[(previous_word, word)] = self.bigram_frequencies.get((previous_word, word),\n",
    "                                                                                                 0) + 1\n",
    "                    if previous_word != SENTENCE_START and word != SENTENCE_END:\n",
    "                        self.unique_bigrams.add((previous_word, word))\n",
    "                previous_word = word\n",
    "        # we subtracted two for the Unigram model as the unigram_frequencies dictionary\n",
    "        # contains values for SENTENCE_START and SENTENCE_END but these need to be included in Bigram\n",
    "        self.unique__bigram_words = len(self.unigram_frequencies)\n",
    "\n",
    "    def calculate_bigram_probabilty(self, previous_word, word):\n",
    "        bigram_word_probability_numerator = self.bigram_frequencies.get((previous_word, word), 0)\n",
    "        bigram_word_probability_denominator = self.unigram_frequencies.get(previous_word, 0)\n",
    "        if self.smoothing:\n",
    "            bigram_word_probability_numerator += 1\n",
    "            bigram_word_probability_denominator += self.unique__bigram_words\n",
    "        return 0.0 if bigram_word_probability_numerator == 0 or bigram_word_probability_denominator == 0 else float(\n",
    "            bigram_word_probability_numerator) / float(bigram_word_probability_denominator)\n",
    "\n",
    "    def calculate_bigram_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        bigram_sentence_probability_log_sum = 0\n",
    "        previous_word = None\n",
    "        for word in sentence:\n",
    "            if previous_word != None:\n",
    "                bigram_word_probability = self.calculate_bigram_probabilty(previous_word, word)\n",
    "                bigram_sentence_probability_log_sum += math.log(bigram_word_probability, 2)\n",
    "            previous_word = word\n",
    "        return math.pow(2,\n",
    "                        bigram_sentence_probability_log_sum) if normalize_probability else bigram_sentence_probability_log_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of unigrams & bigrams\n",
    "def calculate_number_of_unigrams(sentences):\n",
    "    unigram_count = 0\n",
    "    for sentence in sentences:\n",
    "        # remove two for <s> and </s>\n",
    "        unigram_count += len(sentence) - 2\n",
    "    return unigram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_bigrams(sentences):\n",
    "        bigram_count = 0\n",
    "        for sentence in sentences:\n",
    "            # remove one for number of bigrams in sentence\n",
    "            bigram_count += len(sentence) - 1\n",
    "        return bigram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Unigram and Bigram probs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unigram_probs(sorted_vocab_keys, model):\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != SENTENCE_START and vocab_key != SENTENCE_END:\n",
    "            print(\"{}: {}\".format(vocab_key if vocab_key != UNK else \"UNK\",\n",
    "                                       model.calculate_unigram_probability(vocab_key)), end=\" \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bigram_probs(sorted_vocab_keys, model):\n",
    "    print(\"\\t\\t\", end=\"\")\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != SENTENCE_START:\n",
    "            print(vocab_key if vocab_key != UNK else \"UNK\", end=\"\\t\\t\")\n",
    "    print(\"\")\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != SENTENCE_END:\n",
    "            print(vocab_key if vocab_key != UNK else \"UNK\", end=\"\\t\\t\")\n",
    "            for vocab_key_second in sorted_vocab_keys:\n",
    "                if vocab_key_second != SENTENCE_START:\n",
    "                    print(\"{0:.5f}\".format(model.calculate_bigram_probabilty(vocab_key, vocab_key_second)), end=\"\\t\\t\")\n",
    "            print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Calculate perplexty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_perplexity(model, sentences):\n",
    "    unigram_count = calculate_number_of_unigrams(sentences)\n",
    "    sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            sentence_probability_log_sum -= math.log(model.calculate_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            sentence_probability_log_sum -= float('inf')\n",
    "    return math.pow(2, sentence_probability_log_sum / unigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_perplexity(model, sentences):\n",
    "    number_of_bigrams = calculate_number_of_bigrams(sentences)\n",
    "    bigram_sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            bigram_sentence_probability_log_sum -= math.log(model.calculate_bigram_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            bigram_sentence_probability_log_sum -= float('inf')\n",
    "    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Run`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Sample data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_dataset = read_sentences_from_file(\"./sampledata.txt\")\n",
    "# toy_dataset_test = read_sentences_from_file(\"./sampletest.txt\")\n",
    "\n",
    "# toy_dataset_model_unsmoothed = BigramLanguageModel(toy_dataset)\n",
    "# toy_dataset_model_smoothed = BigramLanguageModel(toy_dataset, smoothing=True)\n",
    "\n",
    "# sorted_vocab_keys = toy_dataset_model_unsmoothed.sorted_vocabulary()\n",
    "\n",
    "# print(\"---------------- Sample dataset ---------------\\n\")\n",
    "# print(\"=== UNIGRAM MODEL ===\")\n",
    "# print(\"- Unsmoothed  -\")\n",
    "# print_unigram_probs(sorted_vocab_keys, toy_dataset_model_unsmoothed)\n",
    "# print(\"\\n- Smoothed  -\")\n",
    "# print_unigram_probs(sorted_vocab_keys, toy_dataset_model_smoothed)\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# print(\"=== BIGRAM MODEL ===\")\n",
    "# print(\"- Unsmoothed  -\")\n",
    "# print_bigram_probs(sorted_vocab_keys, toy_dataset_model_unsmoothed)\n",
    "# print(\"- Smoothed  -\")\n",
    "# print_bigram_probs(sorted_vocab_keys, toy_dataset_model_smoothed)\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# print(\"== SENTENCE PROBABILITIES == \")\n",
    "# longest_sentence_len = max([len(\" \".join(sentence)) for sentence in toy_dataset_test]) + 5\n",
    "# print(\"sent\", \" \" * (longest_sentence_len - len(\"sent\") - 2), \"uprob\\t\\tbiprob\")\n",
    "# for sentence in toy_dataset_test:\n",
    "#     sentence_string = \" \".join(sentence)\n",
    "#     print(sentence_string, end=\" \" * (longest_sentence_len - len(sentence_string)))\n",
    "#     print(\"{0:.5f}\".format(toy_dataset_model_smoothed.calculate_sentence_probability(sentence)), end=\"\\t\\t\")\n",
    "#     print(\"{0:.5f}\".format(toy_dataset_model_smoothed.calculate_bigram_sentence_probability(sentence)))        \n",
    "    \n",
    "# print(\"\")\n",
    "\n",
    "# print(\"== TEST PERPLEXITY == \")\n",
    "# print(\"unigram: \", calculate_unigram_perplexity(toy_dataset_model_smoothed, toy_dataset_test))\n",
    "# print(\"bigram: \", calculate_bigram_perplexity(toy_dataset_model_smoothed, toy_dataset_test))\n",
    "\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Actual data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_dataset = read_sentences_from_file(\"train.txt\")\n",
    "actual_dataset_test = read_sentences_from_file(\"test.txt\")\n",
    "\n",
    "uigramLanguageModel= UnigramLanguageModel(actual_dataset)\n",
    "bigramLanguageModel= BigramLanguageModel(actual_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY of train.txt\n",
      "unigram:  753.8242377461315\n",
      "bigram:  16.05992418507916\n"
     ]
    }
   ],
   "source": [
    "print(\"PERPLEXITY of train.txt\")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(uigramLanguageModel, actual_dataset))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(bigramLanguageModel, actual_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY of test.txt\n",
      "unigram:  0.0\n",
      "bigram:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"PERPLEXITY of test.txt\")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(uigramLanguageModel, actual_dataset_test))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(bigramLanguageModel, actual_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBABILITY\n",
      "1.6638831424153624e-83\n",
      "1.6638831424153624e-83\n"
     ]
    }
   ],
   "source": [
    "print(\"PROBABILITY\")\n",
    "print(uigramLanguageModel.calculate_sentence_probability(actual_dataset_test[0]))\n",
    "print(bigramLanguageModel.calculate_sentence_probability(actual_dataset_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'the', 'website', 'and', 'monthly', 'newsletter', 'is', 'run', 'by', 'a', 'sub-committee', 'that', 'is', 'independent', 'to', 'the', 'parish', 'council', 'and', 'is', 'financed', 'through', 'selling', 'advertisement', 'space', 'to', 'local', 'businesses', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# print(actual_dataset_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\An\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\An\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/fuzzytm/\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/5a/d7/f7f93c41fde5b8c1f9d52cc0f9a104a56eca13dc6876c6d2f967ddef88d7/fst-pso-1.8.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022BACF0B310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/5a/d7/f7f93c41fde5b8c1f9d52cc0f9a104a56eca13dc6876c6d2f967ddef88d7/fst-pso-1.8.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022BACF0BE90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/5a/d7/f7f93c41fde5b8c1f9d52cc0f9a104a56eca13dc6876c6d2f967ddef88d7/fst-pso-1.8.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/5a/d7/f7f93c41fde5b8c1f9d52cc0f9a104a56eca13dc6876c6d2f967ddef88d7/fst-pso-1.8.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError('A certificate chain processed, but terminated in a root certificate which is not trusted by the trust provider.'))': /packages/5a/d7/f7f93c41fde5b8c1f9d52cc0f9a104a56eca13dc6876c6d2f967ddef88d7/fst-pso-1.8.1.tar.gz\n",
      "ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/5a/d7/f7f93c41fde5b8c1f9d52cc0f9a104a56eca13dc6876c6d2f967ddef88d7/fst-pso-1.8.1.tar.gz (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022BACF22150>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)'))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\an\\appdata\\roaming\\python\\python311\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Using cached FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\an\\appdata\\roaming\\python\\python311\\site-packages (from FuzzyTM>=0.4.0->gensim) (2.0.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Using cached pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\an\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\an\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.10.1-cp311-cp311-win_amd64.whl.metadata (58 kB)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.24.4-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Using cached simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to read local cache 'C:\\\\Users\\\\An/gensim-data\\\\information.json' during fallback, connect to the Internet and retry",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\An\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\downloader.py:219\u001b[0m, in \u001b[0;36m_load_info\u001b[1;34m(url, encoding)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# We need io.open here because Py2 open doesn't support encoding keyword\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\An/gensim-data\\\\information.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# check available models and datasets\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m info_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(info_datasets)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#>{'corpora': \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#> {'semeval-2016-2017-task3-subtaskBC': \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#>\t {'num_records': -1, 'record_format': 'dict', 'file_size': 6344358, ....}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# information of a particular dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\An\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\downloader.py:268\u001b[0m, in \u001b[0;36minfo\u001b[1;34m(name, show_only_latest, name_only)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, show_only_latest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide the information related to model/dataset.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m \n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43m_load_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\An\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\downloader.py:222\u001b[0m, in \u001b[0;36m_load_info\u001b[1;34m(url, encoding)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to read local cache \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m during fallback, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnect to the Internet and retry\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m cache_path\n\u001b[0;32m    225\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: unable to read local cache 'C:\\\\Users\\\\An/gensim-data\\\\information.json' during fallback, connect to the Internet and retry"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# check available models and datasets\n",
    "info_datasets = api.info()\n",
    "print(info_datasets)\n",
    "#>{'corpora': \n",
    "#> {'semeval-2016-2017-task3-subtaskBC': \n",
    "#>\t {'num_records': -1, 'record_format': 'dict', 'file_size': 6344358, ....}\n",
    "\n",
    "# information of a particular dataset\n",
    "dataset_info = api.info(\"text8\")\n",
    "\n",
    "# load the \"text8\" dataset\n",
    "dataset = api.load(\"text8\")\n",
    "\n",
    "# load a pre-trained model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
